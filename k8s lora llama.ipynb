{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe77b8e-31f5-4a42-b983-d7d6fea8cb30",
      "metadata": {
        "id": "abe77b8e-31f5-4a42-b983-d7d6fea8cb30",
        "outputId": "6098b51f-628e-4a4a-f5fa-a22c502fc93a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6beb957-6398-4ec2-b238-60ba818eb102",
      "metadata": {
        "id": "c6beb957-6398-4ec2-b238-60ba818eb102",
        "outputId": "5a94762d-1254-4cd0-fb3b-f678c7e5b24a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting accelerate>=1.4.0 (from trl)\n",
            "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting datasets>=3.0.0 (from trl)\n",
            "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting transformers>=4.56.1 (from trl)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.8.0.dev20250319+cu128)\n",
            "Collecting huggingface_hub>=0.21.0 (from accelerate>=1.4.0->trl)\n",
            "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting safetensors>=0.4.3 (from accelerate>=1.4.0->trl)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.16.1)\n",
            "Collecting pyarrow>=21.0.0 (from datasets>=3.0.0->trl)\n",
            "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets>=3.0.0->trl)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.3.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
            "Collecting tqdm>=4.66.3 (from datasets>=3.0.0->trl)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting xxhash (from datasets>=3.0.0->trl)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.10.0)\n",
            "Collecting regex!=2019.12.17 (from transformers>=4.56.1->trl)\n",
            "  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.12.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl)\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.8.0.87)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.3.14)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.3.41)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.9.55)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.2.55)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.7.53)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.25.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.55)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.0.11)\n",
            "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.0+git96316ce5)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=2.0.0->accelerate>=1.4.0->trl) (77.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (2.1.5)\n",
            "Downloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
            "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
            "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m171.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m147.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m214.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m262.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
            "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
            "Installing collected packages: xxhash, tqdm, safetensors, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, huggingface_hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, datasets, trl\n",
            "Successfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.1.1 dill-0.4.0 frozenlist-1.7.0 hf-xet-1.1.10 huggingface_hub-0.35.1 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.56.2 trl-0.23.0 xxhash-3.5.0 yarl-1.20.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba2621d2-f2fa-4fd0-8187-c38a60d8ea00",
      "metadata": {
        "id": "ba2621d2-f2fa-4fd0-8187-c38a60d8ea00",
        "outputId": "7c3f206b-3c00-4e5e-8be0-45fe95be8a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.8.0.dev20250319+cu128)\n",
            "Collecting accelerate>=0.21.0 (from peft)\n",
            "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.57)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.8.0.87)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.3.14)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.3.3.41)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.9.55)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.7.2.55)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.5.7.53)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.25.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.55)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.61)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.0.11)\n",
            "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.3.0+git96316ce5)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.13.0->peft) (77.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
            "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
            "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
            "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
            "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
            "Installing collected packages: xxhash, tqdm, safetensors, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, peft, datasets\n",
            "Successfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.1.1 dill-0.4.0 frozenlist-1.7.0 hf-xet-1.1.10 huggingface-hub-0.35.1 multidict-6.6.4 multiprocess-0.70.16 peft-0.17.1 propcache-0.3.2 pyarrow-21.0.0 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.56.2 xxhash-3.5.0 yarl-1.20.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers peft datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab281ac-4b5a-42c4-9d64-1c9df2206f7f",
      "metadata": {
        "id": "0ab281ac-4b5a-42c4-9d64-1c9df2206f7f",
        "outputId": "7a6d3bf7-9741-4ba2-dc27-6fc7e1594b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa854186-a141-405f-ac15-e6d74e1c151a",
      "metadata": {
        "id": "fa854186-a141-405f-ac15-e6d74e1c151a"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34921d3-db45-4ba0-8090-02e34c28f2d2",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e9569287c10e4f30af2857e32ac3aa5a",
            "13842608c8964bc49f2504b8930229cb",
            "b8c8da479e704a0b9158759bbfcd0f6a",
            "f0e48864674d46cea155e07602185942",
            "3eda6b54878145fb9fec3682336cdd23",
            "72d443f1259d48c4a3dd1e9e9945ec4e",
            "efe630e912ac4f9c94c88d3d2fd74afb",
            "e631248a3ed04ef78ad13c79c53b3610",
            "a588a2277093475fb019cf473cb7b4da",
            "7fa85f586a3243b89c2ab359e9b71a26"
          ]
        },
        "id": "a34921d3-db45-4ba0-8090-02e34c28f2d2",
        "outputId": "8dbe0ab2-77da-4256-bd39-733c710da458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading data...\n",
            "Loading tokenizer and model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9569287c10e4f30af2857e32ac3aa5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13842608c8964bc49f2504b8930229cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8c8da479e704a0b9158759bbfcd0f6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0e48864674d46cea155e07602185942",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eda6b54878145fb9fec3682336cdd23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72d443f1259d48c4a3dd1e9e9945ec4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efe630e912ac4f9c94c88d3d2fd74afb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e631248a3ed04ef78ad13c79c53b3610",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a588a2277093475fb019cf473cb7b4da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fa85f586a3243b89c2ab359e9b71a26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded with 3,212,749,824 parameters\n",
            "Preparing datasets...\n",
            "Training samples: 10000\n",
            "Validation samples: 2000\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 10:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.20</td>\n",
              "      <td>2.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.85</td>\n",
              "      <td>1.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.60</td>\n",
              "      <td>1.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed! Model saved to ./k8s-command-model\n",
            "\n",
            "Training completed successfully!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "import os\n",
        "from typing import Dict, List\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from transformers import LogitsProcessor\n",
        "\n",
        "class SafeLogitsProcessor(LogitsProcessor):\n",
        "    def __call__(self, input_ids, scores):\n",
        "        scores = torch.nan_to_num(scores, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
        "        return scores\n",
        "\n",
        "class KubernetesCommandDataset(Dataset):\n",
        "    def __init__(self, questions: List[str], commands: List[str], tokenizer, max_length: int = 512):\n",
        "        self.questions = questions\n",
        "        self.commands = commands\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        command = self.commands[idx]\n",
        "\n",
        "        # Format the input as instruction-following format\n",
        "        prompt = f\"### Instruction:\\nTranslate the following natural language request to a Kubernetes kubectl command:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n{command}\"\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        encoding = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': encoding['input_ids'].squeeze()\n",
        "        }\n",
        "\n",
        "class KubernetesCommandTrainer:\n",
        "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def load_data(self, csv_path: str):\n",
        "        \"\"\"Load and preprocess the CSV data\"\"\"\n",
        "        print(\"Loading data...\")\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        questions = df['question'].tolist()\n",
        "        commands = df['command'].tolist()\n",
        "\n",
        "        valid_pairs = [(q, c) for q, c in zip(questions, commands) if pd.notna(q) and pd.notna(c)]\n",
        "        questions, commands = zip(*valid_pairs)\n",
        "\n",
        "        print(f\"Loaded {len(questions)} training examples\")\n",
        "        return list(questions), list(commands)\n",
        "\n",
        "    def setup_model_and_tokenizer(self):\n",
        "        \"\"\"Initialize the model and tokenizer\"\"\"\n",
        "        print(\"Loading tokenizer and model...\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,  # Use half precision to save memory\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        self.model.gradient_checkpointing_enable()\n",
        "        self.model.config.use_cache = False\n",
        "\n",
        "        print(f\"Model loaded with {self.model.num_parameters():,} parameters\")\n",
        "\n",
        "    def prepare_datasets(self, questions: List[str], commands: List[str], test_size: float = 0.2):\n",
        "        \"\"\"Prepare train and validation datasets\"\"\"\n",
        "        print(\"Preparing datasets...\")\n",
        "\n",
        "        train_q, val_q, train_c, val_c = train_test_split(\n",
        "            questions, commands, test_size=test_size, random_state=42\n",
        "        )\n",
        "\n",
        "        train_dataset = KubernetesCommandDataset(train_q, train_c, self.tokenizer)\n",
        "        val_dataset = KubernetesCommandDataset(val_q, val_c, self.tokenizer)\n",
        "\n",
        "        print(f\"Training samples: {len(train_dataset)}\")\n",
        "        print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, output_dir: str = \"./k8s-command-model\"):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=1,  # Small batch size to fit in memory\n",
        "            per_device_eval_batch_size=1,\n",
        "            gradient_accumulation_steps=8,  # Simulate larger batch size\n",
        "            warmup_steps=100,\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_steps=100,\n",
        "            learning_rate=2e-5,\n",
        "            bf16=True,   # instead of fp16\n",
        "            fp16=False,  # make sure this is off\n",
        "            dataloader_pin_memory=False,\n",
        "            remove_unused_columns=False,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=None,  # Disable wandb/tensorboard\n",
        "            save_total_limit=2\n",
        "        )\n",
        "\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        print(f\"Training completed! Model saved to {output_dir}\")\n",
        "\n",
        "    def inference(self, question: str, model_path: str = None, max_length: int = 100):\n",
        "        \"\"\"Generate kubectl command from natural language question\"\"\"\n",
        "        if model_path and not hasattr(self, 'model'):\n",
        "            self.load_trained_model(model_path)\n",
        "\n",
        "        prompt = f\"### Instruction:\\nTranslate the following natural language request to a Kubernetes kubectl command:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
        "\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                logits_processor=[SafeLogitsProcessor()],\n",
        "            )\n",
        "\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        command = full_response.split(\"### Response:\\n\")[-1].strip()\n",
        "\n",
        "        return command\n",
        "\n",
        "    def load_trained_model(self, model_path: str):\n",
        "        print(f\"Loading trained model from {model_path}...\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.model.config.use_cache = True  # inference wants cache\n",
        "        print(\"Model loaded successfully!\")\n",
        "\n",
        "\n",
        "    def batch_inference(self, questions: List[str], model_path: str = None):\n",
        "        \"\"\"Run inference on multiple questions\"\"\"\n",
        "        if model_path:\n",
        "            self.load_trained_model(model_path)\n",
        "\n",
        "        results = []\n",
        "        for question in questions:\n",
        "            command = self.inference(question)\n",
        "            results.append({\n",
        "                'question': question,\n",
        "                'generated_command': command\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "def main():\n",
        "    trainer = KubernetesCommandTrainer()\n",
        "\n",
        "    csv_file = \"kubernetes_commands.csv\"  # Update this path\n",
        "    questions, commands = trainer.load_data(csv_file)\n",
        "\n",
        "    trainer.setup_model_and_tokenizer()\n",
        "\n",
        "    train_dataset, val_dataset = trainer.prepare_datasets(questions, commands)\n",
        "\n",
        "    trainer.train(train_dataset, val_dataset)\n",
        "\n",
        "    print(\"\\nTraining completed successfully!\")\n",
        "\n",
        "    test_questions = [\n",
        "        \"View the supported API versions\",\n",
        "        \"Display information about the control plane and cluster services\",\n",
        "        \"Print the list of supported API resources\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Testing inference:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for question in test_questions:\n",
        "        command = trainer.inference(question)\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Generated Command: {command}\")\n",
        "\n",
        "def inference_only_example():\n",
        "    \"\"\"Example of using a pre-trained model for inference only\"\"\"\n",
        "    trainer = KubernetesCommandTrainer()\n",
        "\n",
        "    model_path = \"./k8s-command-model\"\n",
        "    trainer.load_trained_model(model_path)\n",
        "\n",
        "    test_questions = [\n",
        "        \"How do I check the cluster information?\",\n",
        "        \"Show me the API versions\",\n",
        "        \"List all supported resources\",\n",
        "        \"Build manifests from current directory\"\n",
        "    ]\n",
        "\n",
        "    print(\"Inference Results:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    results = trainer.batch_inference(test_questions)\n",
        "    for result in results:\n",
        "        print(f\"\\nQ: {result['question']}\")\n",
        "        print(f\"A: {result['generated_command']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e64657-0e45-46fa-b991-37655186f53e",
      "metadata": {
        "id": "79e64657-0e45-46fa-b991-37655186f53e",
        "outputId": "f854b8fe-5622-4c5f-e2cf-99190aea086f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.78it/s]\n",
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "\n",
            "--- Generated ---\n",
            "\n",
            "[{'role': 'user', 'content': 'User question: Kubernetes command to Print the list of supported namespaced resources\\n'}, {'role': 'assistant', 'content': 'kubectl api-resources'}]\n"
          ]
        }
      ],
      "source": [
        "# !python train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db7bdf5c-d20a-4d19-9970-30798c84860a",
      "metadata": {
        "id": "db7bdf5c-d20a-4d19-9970-30798c84860a",
        "outputId": "71e1ab1a-66b7-4dd0-93a3-33ea94c57e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Sep 24 16:37:33 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A40                     On  |   00000000:53:00.0 Off |                    0 |\n",
            "|  0%   45C    P0            209W /  300W |   27707MiB /  46068MiB |     86%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e340c5-38f1-4ea4-8ffb-7ac0cacfce4f",
      "metadata": {
        "id": "80e340c5-38f1-4ea4-8ffb-7ac0cacfce4f",
        "outputId": "67a0f06c-89e5-4251-f7bc-60404669bd81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'k8s lora 23-9 Claude (3).ipynb'   k8s-command-model   kubernetes_commands.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbcfd92a-ec3d-4209-8b9a-40be9b4d7491",
      "metadata": {
        "id": "fbcfd92a-ec3d-4209-8b9a-40be9b4d7491",
        "outputId": "9a0915bc-83ad-4e79-b62a-60bfd81d2cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Untitled.ipynb  'k8s lora 23-9 Claude.ipynb'   kubernetes_commands.csv\n",
            " inf.py\t\t  k8s-command-model\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21db5cc4-9198-4b97-a55e-48aee0de8cf9",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4de472283bfd4b379218d88188d5aa9e"
          ]
        },
        "id": "21db5cc4-9198-4b97-a55e-48aee0de8cf9",
        "outputId": "639f0521-2e11-4749-f8c7-4fdc571b4998"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4de472283bfd4b379218d88188d5aa9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "Translate the following natural language request to a Kubernetes kubectl command:\n",
            "\n",
            "### Input:\n",
            "List all pods in the default namespace\n",
            "\n",
            "### Response:\n",
            "```bash\n",
            "kubectl get pods --namespace=default\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "PEFT_PATH = \"/workspace/k8s-command-model/checkpoint-150\"\n",
        "\n",
        "def load_model():\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # Load base model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        _ = torch.load(PEFT_PATH, map_location=\"cpu\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "def run_inference(prompt: str, max_new_tokens: int = 128):\n",
        "    tokenizer, model = load_model()\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"cuda:0\",\n",
        "    )\n",
        "\n",
        "    outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return outputs[0][\"generated_text\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prompt = \"\"\"### Instruction:\n",
        "Translate the following natural language request to a Kubernetes kubectl command:\n",
        "\n",
        "### Input:\n",
        "List all pods in the default namespace\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    result = run_inference(prompt)\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e0e2f8-cb55-4137-b4e1-95c7c3a8d53f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2694a0f257fc4c5ea4e234b1e1e03cd8"
          ]
        },
        "id": "13e0e2f8-cb55-4137-b4e1-95c7c3a8d53f",
        "outputId": "42561a83-3740-4a23-cdde-ab9493149ae8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2694a0f257fc4c5ea4e234b1e1e03cd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "Translate the following natural language request to a Kubernetes kubectl command:\n",
            "\n",
            "### Input:\n",
            "Display detailed information about a specific pod\n",
            "\n",
            "### Response:\n",
            "```bash\n",
            "kubectl get pod <pod_name> -o wide\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"### Instruction:\n",
        "Translate the following natural language request to a Kubernetes kubectl command:\n",
        "\n",
        "### Input:\n",
        "Display detailed information about a specific pod\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "result = run_inference(prompt)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50df3810-dda2-4f57-94d1-77cc9b15b34c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ddd8882e2f0441ba9ec11b06edcc4150"
          ]
        },
        "id": "50df3810-dda2-4f57-94d1-77cc9b15b34c",
        "outputId": "5fb8c4dd-408b-42e2-dab5-10c92f3da19c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddd8882e2f0441ba9ec11b06edcc4150",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "Translate the following natural language request to a Kubernetes kubectl command:\n",
            "\n",
            "### Input:\n",
            "View logs from the first container of a job named \"batch-processing-job\"\n",
            "\n",
            "### Response:\n",
            "```\n",
            "kubectl logs job/batch-processing-job -c 0\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"### Instruction:\n",
        "Translate the following natural language request to a Kubernetes kubectl command:\n",
        "\n",
        "### Input:\n",
        "View logs from the first container of a job named \"batch-processing-job\"\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "result = run_inference(prompt)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd204a60-22eb-4163-95b3-bdc83cb82e4e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "362751c9093f4dfd8d7bd04531fac1a6",
            "b3fb2beb1a8143ac82eabc97a4f7362e"
          ]
        },
        "id": "dd204a60-22eb-4163-95b3-bdc83cb82e4e",
        "outputId": "30a47749-d63d-4442-ab5f-6529e359206f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "362751c9093f4dfd8d7bd04531fac1a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3fb2beb1a8143ac82eabc97a4f7362e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "# --- config ---\n",
        "repo_id       = \"tarun122/k8s-lora-final-2\"\n",
        "folder_path   = \"/workspace/k8s-command-model\"\n",
        "repo_type     = \"model\"\n",
        "private       = False\n",
        "commit_msg    = \"Upload folder\"\n",
        "path_in_repo  = \"\"\n",
        "\n",
        "create_repo(repo_id, repo_type=repo_type, private=private, exist_ok=True)\n",
        "\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    repo_type=repo_type,\n",
        "    folder_path=folder_path,\n",
        "    path_in_repo=path_in_repo,\n",
        "    commit_message=commit_msg,\n",
        "    # token=None\n",
        ")\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d81777de-d1f3-435b-853e-5ac703436ec2",
      "metadata": {
        "id": "d81777de-d1f3-435b-853e-5ac703436ec2",
        "outputId": "0b85225f-ddaf-4393-b6bf-297742eb2c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 6.1G\n",
            "2.9M drwxrwxrwx 2 root root 2.9M Sep 24 16:33 checkpoint-100\n",
            " 512 -rw-rw-rw- 1 root root  184 Sep 24 16:36 generation_config.json\n",
            "1.0K -rw-rw-rw- 1 root root  867 Sep 24 16:36 config.json\n",
            "2.9M drwxrwxrwx 2 root root 2.9M Sep 24 16:36 checkpoint-150\n",
            "4.7G -rw-rw-rw- 1 root root 4.7G Sep 24 16:37 model-00001-of-00002.safetensors\n",
            "1.4G -rw-rw-rw- 1 root root 1.4G Sep 24 16:37 model-00002-of-00002.safetensors\n",
            "6.0K -rw-rw-rw- 1 root root 5.7K Sep 24 16:37 training_args.bin\n",
            " 50K -rw-rw-rw- 1 root root  50K Sep 24 16:37 tokenizer_config.json\n",
            " 17M -rw-rw-rw- 1 root root  17M Sep 24 16:37 tokenizer.json\n",
            " 512 -rw-rw-rw- 1 root root  325 Sep 24 16:37 special_tokens_map.json\n",
            " 21K -rw-rw-rw- 1 root root  21K Sep 24 16:37 model.safetensors.index.json\n",
            "4.0K -rw-rw-rw- 1 root root 3.8K Sep 24 16:37 chat_template.jinja\n"
          ]
        }
      ],
      "source": [
        "!ls -lstrh /workspace/k8s-command-model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35423de9-70ad-41cd-a0f6-837709ee0323",
      "metadata": {
        "id": "35423de9-70ad-41cd-a0f6-837709ee0323"
      },
      "source": [
        "# Train 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b136305e-1203-486f-82f6-83c848017945",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "340935f7c87f4327a2077c88106f1520"
          ]
        },
        "id": "b136305e-1203-486f-82f6-83c848017945",
        "outputId": "762ad863-d1c5-4b19-8424-e8a4def4006c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading data...\n",
            "Loading tokenizer and model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "340935f7c87f4327a2077c88106f1520",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded with 8,030,269,440 parameters\n",
            "Vocabulary size: 128257\n",
            "Pad token: <PAD>\n",
            "EOS token: <|eot_id|>\n",
            "Preparing datasets...\n",
            "Training samples: 399\n",
            "Validation samples: 100\n",
            "Starting memory-efficient training...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "KubernetesCommandTrainer.train_memory_efficient.<locals>.CustomTrainer.training_step() takes 3 positional arguments but 4 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 448\u001b[39m\n\u001b[32m    444\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mgenerated_command\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# For training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     \u001b[38;5;66;03m# Uncomment below for inference only\u001b[39;00m\n\u001b[32m    451\u001b[39m     \u001b[38;5;66;03m# inference_only_example()\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 402\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    399\u001b[39m train_dataset, val_dataset = trainer.prepare_datasets(questions, commands)\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_memory_efficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# Test inference\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 320\u001b[39m, in \u001b[36mKubernetesCommandTrainer.train_memory_efficient\u001b[39m\u001b[34m(self, train_dataset, val_dataset, output_dir)\u001b[39m\n\u001b[32m    311\u001b[39m trainer = CustomTrainer(\n\u001b[32m    312\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    313\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m     data_collator=data_collator,\n\u001b[32m    317\u001b[39m )\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[32m    323\u001b[39m trainer.save_model()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[31mTypeError\u001b[39m: KubernetesCommandTrainer.train_memory_efficient.<locals>.CustomTrainer.training_step() takes 3 positional arguments but 4 were given"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "import os\n",
        "from typing import Dict, List\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class KubernetesCommandDataset(Dataset):\n",
        "    def __init__(self, questions: List[str], commands: List[str], tokenizer, max_length: int = 512):\n",
        "        self.questions = questions\n",
        "        self.commands = commands\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        command = self.commands[idx]\n",
        "\n",
        "        # Format the input as instruction-following format with clear separators\n",
        "        prompt = f\"<|begin_of_text|>### Instruction:\\nTranslate the following natural language request to a Kubernetes kubectl command:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n{command}<|end_of_text|>\"\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        encoding = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=False,  # Don't pad here, let the data collator handle it\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # Create labels - mask the instruction part, only train on the response\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Find the start of the response section\n",
        "        response_start_text = \"### Response:\\n\"\n",
        "        response_tokens = self.tokenizer.encode(response_start_text, add_special_tokens=False)\n",
        "\n",
        "        # Find where the response starts in the input_ids\n",
        "        for i in range(len(input_ids) - len(response_tokens) + 1):\n",
        "            if input_ids[i:i+len(response_tokens)].tolist() == response_tokens:\n",
        "                # Mask everything before the response (set to -100)\n",
        "                labels[:i+len(response_tokens)] = -100\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "class KubernetesCommandTrainer:\n",
        "    def __init__(self, model_name: str = \"meta-llama/Llama-3.1-8B-Instruct\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def load_data(self, csv_path: str):\n",
        "        \"\"\"Load and preprocess the CSV data\"\"\"\n",
        "        print(\"Loading data...\")\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Extract questions and commands\n",
        "        questions = df['question'].tolist()\n",
        "        commands = df['command'].tolist()\n",
        "\n",
        "        # Remove any rows with NaN values\n",
        "        valid_pairs = [(q, c) for q, c in zip(questions, commands) if pd.notna(q) and pd.notna(c)]\n",
        "        questions, commands = zip(*valid_pairs)\n",
        "\n",
        "        print(f\"Loaded {len(questions)} training examples\")\n",
        "        return list(questions), list(commands)\n",
        "\n",
        "    def setup_model_and_tokenizer(self):\n",
        "        \"\"\"Initialize the model and tokenizer\"\"\"\n",
        "        print(\"Loading tokenizer and model...\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Set proper padding token (use a different token than EOS)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            # Try to use existing special tokens first\n",
        "            if hasattr(self.tokenizer, 'unk_token') and self.tokenizer.unk_token:\n",
        "                self.tokenizer.pad_token = self.tokenizer.unk_token\n",
        "            else:\n",
        "                # Add a new padding token\n",
        "                self.tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
        "\n",
        "        # Load model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,  # Use half precision to save memory\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Resize embeddings if we added new tokens\n",
        "        if self.tokenizer.pad_token == '<PAD>':\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        # Enable gradient checkpointing to save memory\n",
        "        self.model.gradient_checkpointing_enable()\n",
        "        self.model.config.use_cache = False\n",
        "\n",
        "\n",
        "        print(f\"Model loaded with {self.model.num_parameters():,} parameters\")\n",
        "        print(f\"Vocabulary size: {len(self.tokenizer)}\")\n",
        "        print(f\"Pad token: {self.tokenizer.pad_token}\")\n",
        "        print(f\"EOS token: {self.tokenizer.eos_token}\")\n",
        "\n",
        "    def prepare_datasets(self, questions: List[str], commands: List[str], test_size: float = 0.2):\n",
        "        \"\"\"Prepare train and validation datasets\"\"\"\n",
        "        print(\"Preparing datasets...\")\n",
        "\n",
        "        # Split data\n",
        "        train_q, val_q, train_c, val_c = train_test_split(\n",
        "            questions, commands, test_size=test_size, random_state=42\n",
        "        )\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = KubernetesCommandDataset(train_q, train_c, self.tokenizer)\n",
        "        val_dataset = KubernetesCommandDataset(val_q, val_c, self.tokenizer)\n",
        "\n",
        "        print(f\"Training samples: {len(train_dataset)}\")\n",
        "        print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, output_dir: str = \"./k8s-command-model\"):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=1,  # Small batch size to fit in memory\n",
        "            per_device_eval_batch_size=1,\n",
        "            gradient_accumulation_steps=8,  # Simulate larger batch size\n",
        "            warmup_steps=100,\n",
        "            logging_steps=10,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=50,\n",
        "            save_steps=100,\n",
        "            learning_rate=2e-5,\n",
        "            fp16=True,  # Use mixed precision training\n",
        "            dataloader_pin_memory=False,\n",
        "            remove_unused_columns=False,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=None,  # Disable wandb/tensorboard\n",
        "            save_total_limit=2,\n",
        "        )\n",
        "\n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the final model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        print(f\"Training completed! Model saved to {output_dir}\")\n",
        "    def train_memory_efficient(self, train_dataset, val_dataset, output_dir: str = \"./k8s-command-model\"):\n",
        "        \"\"\"Memory-efficient training with proper FP16 handling\"\"\"\n",
        "        print(\"Starting memory-efficient training...\")\n",
        "\n",
        "        # Training arguments with proper FP16 setup\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=1,  # Very small batch size\n",
        "            per_device_eval_batch_size=1,\n",
        "            gradient_accumulation_steps=8,  # Simulate larger batches\n",
        "            warmup_steps=50,\n",
        "            logging_steps=5,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=25,\n",
        "            save_steps=50,\n",
        "            learning_rate=1e-5,\n",
        "            fp16=True,  # Keep FP16 for memory savings\n",
        "            fp16_full_eval=True,\n",
        "            dataloader_pin_memory=False,\n",
        "            remove_unused_columns=False,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            report_to=None,\n",
        "            save_total_limit=2,\n",
        "            # Remove max_grad_norm to avoid FP16 conflicts\n",
        "            # max_grad_norm=1.0,  # Commented out\n",
        "            warmup_ratio=0.1,\n",
        "            optim=\"adamw_torch\",\n",
        "            adam_epsilon=1e-6,\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "        )\n",
        "\n",
        "        # Custom data collator (same as before)\n",
        "        class CustomDataCollator:\n",
        "            def __init__(self, tokenizer):\n",
        "                self.tokenizer = tokenizer\n",
        "\n",
        "            def __call__(self, features):\n",
        "                batch = {}\n",
        "                max_length = max([len(f['input_ids']) for f in features])\n",
        "\n",
        "                batch['input_ids'] = []\n",
        "                batch['attention_mask'] = []\n",
        "                batch['labels'] = []\n",
        "\n",
        "                for feature in features:\n",
        "                    input_ids = feature['input_ids']\n",
        "                    attention_mask = feature['attention_mask']\n",
        "                    labels = feature['labels']\n",
        "\n",
        "                    # Pad sequences\n",
        "                    padding_length = max_length - len(input_ids)\n",
        "\n",
        "                    # Pad input_ids and attention_mask\n",
        "                    padded_input_ids = torch.cat([\n",
        "                        input_ids,\n",
        "                        torch.full((padding_length,), self.tokenizer.pad_token_id)\n",
        "                    ])\n",
        "                    padded_attention_mask = torch.cat([\n",
        "                        attention_mask,\n",
        "                        torch.zeros(padding_length)\n",
        "                    ])\n",
        "\n",
        "                    # Pad labels (use -100 for padded positions)\n",
        "                    padded_labels = torch.cat([\n",
        "                        labels,\n",
        "                        torch.full((padding_length,), -100)\n",
        "                    ])\n",
        "\n",
        "                    batch['input_ids'].append(padded_input_ids)\n",
        "                    batch['attention_mask'].append(padded_attention_mask)\n",
        "                    batch['labels'].append(padded_labels)\n",
        "\n",
        "                # Stack tensors\n",
        "                batch['input_ids'] = torch.stack(batch['input_ids'])\n",
        "                batch['attention_mask'] = torch.stack(batch['attention_mask'])\n",
        "                batch['labels'] = torch.stack(batch['labels'])\n",
        "\n",
        "                return batch\n",
        "\n",
        "        data_collator = CustomDataCollator(self.tokenizer)\n",
        "\n",
        "        # Custom Trainer class to handle gradient clipping manually\n",
        "        class CustomTrainer(Trainer):\n",
        "            def training_step(self, model, inputs):\n",
        "                model.train()\n",
        "                inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "                with self.compute_loss_context_manager():\n",
        "                    loss = self.compute_loss(model, inputs)\n",
        "\n",
        "                if self.args.n_gpu > 1:\n",
        "                    loss = loss.mean()\n",
        "\n",
        "                if self.args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "                if self.use_apex:\n",
        "                    with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    self.accelerator.backward(loss)\n",
        "\n",
        "                return loss.detach() / self.args.gradient_accumulation_steps\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = CustomTrainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the final model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        print(f\"Training completed! Model saved to {output_dir}\")\n",
        "\n",
        "    def inference(self, question: str, model_path: str = None, max_length: int = 100):\n",
        "        \"\"\"Generate kubectl command from natural language question\"\"\"\n",
        "        if model_path and not hasattr(self, 'model'):\n",
        "            self.load_trained_model(model_path)\n",
        "\n",
        "        prompt = f\"### Instruction:\\nTranslate the following natural language request to a Kubernetes kubectl command:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + max_length,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # Decode and extract the command\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        command = full_response.split(\"### Response:\\n\")[-1].strip()\n",
        "\n",
        "        return command\n",
        "\n",
        "    def load_trained_model(self, model_path: str):\n",
        "        \"\"\"Load a previously trained model\"\"\"\n",
        "        print(f\"Loading trained model from {model_path}...\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(\"Model loaded successfully!\")\n",
        "\n",
        "    def batch_inference(self, questions: List[str], model_path: str = None):\n",
        "        \"\"\"Run inference on multiple questions\"\"\"\n",
        "        if model_path:\n",
        "            self.load_trained_model(model_path)\n",
        "\n",
        "        results = []\n",
        "        for question in questions:\n",
        "            command = self.inference(question)\n",
        "            results.append({\n",
        "                'question': question,\n",
        "                'generated_command': command\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "def main():\n",
        "    # Initialize trainer\n",
        "    trainer = KubernetesCommandTrainer()\n",
        "\n",
        "    # Load data (replace with your CSV file path)\n",
        "    csv_file = \"kubernetes_commands.csv\"  # Update this path\n",
        "    questions, commands = trainer.load_data(csv_file)\n",
        "\n",
        "    # Setup model and tokenizer\n",
        "    trainer.setup_model_and_tokenizer()\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset, val_dataset = trainer.prepare_datasets(questions, commands)\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train_memory_efficient(train_dataset, val_dataset)\n",
        "\n",
        "    print(\"\\nTraining completed successfully!\")\n",
        "\n",
        "    # Test inference\n",
        "    test_questions = [\n",
        "        \"View the supported API versions\",\n",
        "        \"Display information about the control plane and cluster services\",\n",
        "        \"Print the list of supported API resources\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Testing inference:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for question in test_questions:\n",
        "        command = trainer.inference(question)\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Generated Command: {command}\")\n",
        "\n",
        "def inference_only_example():\n",
        "    \"\"\"Example of using a pre-trained model for inference only\"\"\"\n",
        "    trainer = KubernetesCommandTrainer()\n",
        "\n",
        "    # Load the trained model (update path as needed)\n",
        "    model_path = \"./k8s-command-model\"\n",
        "    trainer.load_trained_model(model_path)\n",
        "\n",
        "    # Test questions\n",
        "    test_questions = [\n",
        "        \"How do I check the cluster information?\",\n",
        "        \"Show me the API versions\",\n",
        "        \"List all supported resources\",\n",
        "        \"Build manifests from current directory\"\n",
        "    ]\n",
        "\n",
        "    print(\"Inference Results:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    results = trainer.batch_inference(test_questions)\n",
        "    for result in results:\n",
        "        print(f\"\\nQ: {result['question']}\")\n",
        "        print(f\"A: {result['generated_command']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For training\n",
        "    main()\n",
        "\n",
        "    # Uncomment below for inference only\n",
        "    # inference_only_example()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcea5de4-2948-4872-af94-b5d60355474c",
      "metadata": {
        "id": "bcea5de4-2948-4872-af94-b5d60355474c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "KQlDx7R1W8j-",
      "metadata": {
        "id": "KQlDx7R1W8j-"
      },
      "source": [
        "# testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "wMJOwcGmW-oB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "wMJOwcGmW-oB",
        "outputId": "dc3bc6af-011f-4298-820a-62c865ea5b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "EVALUATION RESULTS\n",
            "==================================================\n",
            "Average BLEU Score: 0.7600\n",
            "Average ROUGE-L Score: 0.9500\n",
            "Average Edit Distance: 1.50\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import Levenshtein\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Install required packages first (run these in separate cells if needed):\n",
        "# !pip install transformers accelerate bitsandbytes\n",
        "# !pip install rouge-score\n",
        "# !pip install python-Levenshtein\n",
        "# !pip install wordcloud\n",
        "\n",
        "# Load model and tokenizer for Llama 3.1 8B\n",
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  # Base model\n",
        "lora_path = \"/workspace/lorasaved\"  # Your LoRA checkpoint\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalSM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load LoRA weights\n",
        "from peft import PeftModel\n",
        "model = PeftModel.from_pretrained(model, lora_path)\n",
        "\n",
        "def predict(text, max_length=512):\n",
        "    # Format for instruction following\n",
        "    prompt = f\"<|start_header_id|>user<|end_header_id|>\\n\\n{text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids.to(model.device),\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            attention_mask=inputs.attention_mask.to(model.device)\n",
        "        )\n",
        "\n",
        "    # Decode only the generated part (after the prompt)\n",
        "    generated_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Clean up the response\n",
        "    response = response.split('<|eot_id|>')[0].strip()\n",
        "    return response\n",
        "\n",
        "# Alternative simpler prediction function\n",
        "def predict_simple(text, max_length=512):\n",
        "    # Simple chat template format\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": text}\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs.to(model.device),\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Remove the prompt from response\n",
        "    response = response.replace(prompt, \"\").strip()\n",
        "    return response\n",
        "\n",
        "# Load test data\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/fullcommand2.csv\")\n",
        "df_test = df_test[['question', 'command']].dropna().head(21000)\n",
        "\n",
        "# Split to get the same test set (80/20 split)\n",
        "from sklearn.model_selection import train_test_split\n",
        "_, test_df = train_test_split(df_test, test_size=0.2, random_state=42)\n",
        "\n",
        "# For faster testing, you might want to use a smaller subset\n",
        "# test_df = test_df.head(100)  # Uncomment for quick testing\n",
        "\n",
        "# Initialize metrics\n",
        "bleu_scores = []\n",
        "rouge_scores = []\n",
        "exact_matches = 0\n",
        "edit_distances = []\n",
        "all_predicted_tokens = []\n",
        "all_reference_tokens = []\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "smoothing = SmoothingFunction().method1\n",
        "\n",
        "print(\"Evaluating Llama 3.1 8B Instruct + LoRA on test set...\")\n",
        "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    question = row['question']\n",
        "    reference = row['command']\n",
        "\n",
        "    try:\n",
        "        # Get prediction - try both methods\n",
        "        predicted = predict_simple(question)\n",
        "\n",
        "        # Clean up prediction - remove any extra text after the command\n",
        "        predicted = predicted.split('\\n')[0].strip()\n",
        "\n",
        "        # Tokenize for BLEU\n",
        "        reference_tokens = reference.split()\n",
        "        predicted_tokens = predicted.split()\n",
        "\n",
        "        # Store tokens for word cloud\n",
        "        all_predicted_tokens.extend(predicted_tokens)\n",
        "        all_reference_tokens.extend(reference_tokens)\n",
        "\n",
        "        # BLEU Score\n",
        "        bleu = sentence_bleu([reference_tokens], predicted_tokens, smoothing_function=smoothing)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # ROUGE-L Score\n",
        "        rouge = scorer.score(reference, predicted)\n",
        "        rouge_scores.append(rouge['rougeL'].fmeasure)\n",
        "\n",
        "        # Exact Match\n",
        "        if predicted.strip() == reference.strip():\n",
        "            exact_matches += 1\n",
        "\n",
        "        # Edit Distance (Levenshtein)\n",
        "        edit_dist = Levenshtein.distance(reference, predicted)\n",
        "        edit_distances.append(edit_dist)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "        # Append zeros for failed cases\n",
        "        bleu_scores.append(0.0)\n",
        "        rouge_scores.append(0.0)\n",
        "        edit_distances.append(len(reference))\n",
        "\n",
        "# Calculate aggregate metrics\n",
        "avg_bleu = np.mean(bleu_scores)\n",
        "avg_rouge = np.mean(rouge_scores)\n",
        "exact_match_accuracy = exact_matches / len(test_df)\n",
        "avg_edit_distance = np.mean(edit_distances)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LLAMA 3.1 8B INSTRUCT + LoRA EVALUATION RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-L Score: {avg_rouge:.4f}\")\n",
        "print(f\"Exact Match Accuracy: {exact_match_accuracy:.4f} ({exact_matches}/{len(test_df)})\")\n",
        "print(f\"Average Edit Distance: {avg_edit_distance:.2f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ============================================\n",
        "# VISUALIZATIONS (same as before)\n",
        "# ============================================\n",
        "\n",
        "# 1. Metrics Bar Chart\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "metrics = ['BLEU', 'ROUGE-L', 'Exact Match', 'Normalized\\nEdit Distance']\n",
        "values = [avg_bleu, avg_rouge, exact_match_accuracy, 1 - (avg_edit_distance / 100)]  # Normalize edit distance\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "bars = ax.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Llama 3.1 8B + LoRA Performance Metrics', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim(0, 1)\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.3f}',\n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('llama_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 2. Distribution Plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# BLEU Distribution\n",
        "axes[0, 0].hist(bleu_scores, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].axvline(avg_bleu, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_bleu:.3f}')\n",
        "axes[0, 0].set_xlabel('BLEU Score', fontsize=11, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "axes[0, 0].set_title('BLEU Score Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# ROUGE-L Distribution\n",
        "axes[0, 1].hist(rouge_scores, bins=30, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].axvline(avg_rouge, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_rouge:.3f}')\n",
        "axes[0, 1].set_xlabel('ROUGE-L Score', fontsize=11, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "axes[0, 1].set_title('ROUGE-L Score Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Edit Distance Distribution\n",
        "axes[1, 0].hist(edit_distances, bins=30, color='#f39c12', alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].axvline(avg_edit_distance, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_edit_distance:.2f}')\n",
        "axes[1, 0].set_xlabel('Edit Distance', fontsize=11, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "axes[1, 0].set_title('Edit Distance Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Exact Match Pie Chart\n",
        "em_labels = ['Exact Match', 'Non-Match']\n",
        "em_values = [exact_matches, len(test_df) - exact_matches]\n",
        "em_colors = ['#2ecc71', '#e67e22']\n",
        "axes[1, 1].pie(em_values, labels=em_labels, autopct='%1.1f%%', colors=em_colors,\n",
        "               startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
        "axes[1, 1].set_title(f'Exact Match Results\\n(Total: {len(test_df)} samples)',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('llama_metrics_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 3. Word Cloud of Common Tokens in Predictions\n",
        "print(\"\\nGenerating word cloud...\")\n",
        "predicted_text = ' '.join(all_predicted_tokens)\n",
        "\n",
        "wordcloud = WordCloud(width=1600, height=800,\n",
        "                      background_color='white',\n",
        "                      colormap='viridis',\n",
        "                      max_words=100,\n",
        "                      relative_scaling=0.5,\n",
        "                      min_font_size=10).generate(predicted_text)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Predicted Command Tokens - Llama 3.1 8B + LoRA', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('llama_wordcloud_predictions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 4. Top 20 Most Common Tokens\n",
        "token_counts = Counter(all_predicted_tokens)\n",
        "top_tokens = token_counts.most_common(20)\n",
        "tokens, counts = zip(*top_tokens)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.barh(range(len(tokens)), counts, color=plt.cm.viridis(np.linspace(0, 1, len(tokens))))\n",
        "plt.yticks(range(len(tokens)), tokens)\n",
        "plt.xlabel('Frequency', fontsize=12, fontweight='bold')\n",
        "plt.title('Top 20 Most Common Tokens in Llama 3.1 8B + LoRA Predictions', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Add count labels\n",
        "for i, (token, count) in enumerate(zip(tokens, counts)):\n",
        "    plt.text(count, i, f' {count}', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('llama_top_tokens.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 5. Save detailed results to CSV\n",
        "# Regenerate predictions for consistent results\n",
        "final_predictions = []\n",
        "for question in tqdm(test_df['question'].values, desc=\"Generating final predictions\"):\n",
        "    final_predictions.append(predict_simple(question))\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'question': test_df['question'].values,\n",
        "    'reference': test_df['command'].values,\n",
        "    'predicted': final_predictions,\n",
        "    'bleu_score': bleu_scores,\n",
        "    'rouge_l_score': rouge_scores,\n",
        "    'edit_distance': edit_distances,\n",
        "    'exact_match': [1 if pred.strip() == ref.strip() else 0\n",
        "                    for pred, ref in zip(final_predictions, test_df['command'].values)]\n",
        "})\n",
        "\n",
        "results_df.to_csv('llama_evaluation_results.csv', index=False)\n",
        "print(\"\\nDetailed results saved to 'llama_evaluation_results.csv'\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DETAILED STATISTICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"BLEU Score - Min: {min(bleu_scores):.4f}, Max: {max(bleu_scores):.4f}, Std: {np.std(bleu_scores):.4f}\")\n",
        "print(f\"ROUGE-L Score - Min: {min(rouge_scores):.4f}, Max: {max(rouge_scores):.4f}, Std: {np.std(rouge_scores):.4f}\")\n",
        "print(f\"Edit Distance - Min: {min(edit_distances)}, Max: {max(edit_distances)}, Std: {np.std(edit_distances):.2f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Example predictions\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EXAMPLE PREDICTIONS\")\n",
        "print(\"=\"*50)\n",
        "for i in range(min(5, len(test_df))):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Question: {test_df.iloc[i]['question']}\")\n",
        "    print(f\"Reference: {test_df.iloc[i]['command']}\")\n",
        "    print(f\"Predicted: {final_predictions[i]}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s7ctnWQpXAVH",
      "metadata": {
        "id": "s7ctnWQpXAVH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "2651f786a81858a092aff2f04d46be6f7750183c4f3dbcab9f4aff1f8952553f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
