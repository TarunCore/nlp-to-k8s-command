{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe77b8e-31f5-4a42-b983-d7d6fea8cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install pandas scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6beb957-6398-4ec2-b238-60ba818eb102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting accelerate>=1.4.0 (from trl)\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets>=3.0.0 (from trl)\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers>=4.56.1 (from trl)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (2.8.0.dev20250319+cu128)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=1.4.0->trl)\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=1.4.0->trl)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.16.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=3.0.0->trl)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets>=3.0.0->trl)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets>=3.0.0->trl)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets>=3.0.0->trl)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.10.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.56.1->trl)\n",
      "  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=2.0.0->accelerate>=1.4.0->trl) (77.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (2.1.5)\n",
      "Downloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m171.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m147.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m214.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m262.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: xxhash, tqdm, safetensors, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, huggingface_hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, datasets, trl\n",
      "Successfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.1.1 dill-0.4.0 frozenlist-1.7.0 hf-xet-1.1.10 huggingface_hub-0.35.1 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.56.2 trl-0.23.0 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install trl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2621d2-f2fa-4fd0-8187-c38a60d8ea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.8.0.dev20250319+cu128)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.13.0->peft) (77.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: xxhash, tqdm, safetensors, regex, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, peft, datasets\n",
      "Successfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.1.1 dill-0.4.0 frozenlist-1.7.0 hf-xet-1.1.10 huggingface-hub-0.35.1 multidict-6.6.4 multiprocess-0.70.16 peft-0.17.1 propcache-0.3.2 pyarrow-21.0.0 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.56.2 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab281ac-4b5a-42c4-9d64-1c9df2206f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa854186-a141-405f-ac15-e6d74e1c151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34921d3-db45-4ba0-8090-02e34c28f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9569287c10e4f30af2857e32ac3aa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13842608c8964bc49f2504b8930229cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c8da479e704a0b9158759bbfcd0f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e48864674d46cea155e07602185942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eda6b54878145fb9fec3682336cdd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d443f1259d48c4a3dd1e9e9945ec4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe630e912ac4f9c94c88d3d2fd74afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e631248a3ed04ef78ad13c79c53b3610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a588a2277093475fb019cf473cb7b4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa85f586a3243b89c2ab359e9b71a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 3,212,749,824 parameters\n",
      "Preparing datasets...\n",
      "Training samples: 10000\n",
      "Validation samples: 2000\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 10:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Model saved to ./k8s-command-model\n",
      "\n",
      "Training completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "class SafeLogitsProcessor(LogitsProcessor):\n",
    "    def __call__(self, input_ids, scores):\n",
    "        scores = torch.nan_to_num(scores, nan=-1e9, posinf=1e9, neginf=-1e9)\n",
    "        return scores\n",
    "\n",
    "class KubernetesCommandDataset(Dataset):\n",
    "    def __init__(self, questions: List[str], commands: List[str], tokenizer, max_length: int = 512):\n",
    "        self.questions = questions\n",
    "        self.commands = commands\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        command = self.commands[idx]\n",
    "        \n",
    "        # Format the input as instruction-following format\n",
    "        prompt = f\"### Instruction:\\nTranslate the following natural language request to a Kubernetes kubectl command:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n{command}\"\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': encoding['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "class KubernetesCommandTrainer:\n",
    "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def load_data(self, csv_path: str):\n",
    "        \"\"\"Load and preprocess the CSV data\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        questions = df['question'].tolist()\n",
    "        commands = df['command'].tolist()\n",
    "        \n",
    "        valid_pairs = [(q, c) for q, c in zip(questions, commands) if pd.notna(q) and pd.notna(c)]\n",
    "        questions, commands = zip(*valid_pairs)\n",
    "        \n",
    "        print(f\"Loaded {len(questions)} training examples\")\n",
    "        return list(questions), list(commands)\n",
    "    \n",
    "    def setup_model_and_tokenizer(self):\n",
    "        \"\"\"Initialize the model and tokenizer\"\"\"\n",
    "        print(\"Loading tokenizer and model...\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.model.config.use_cache = False \n",
    "        \n",
    "        print(f\"Model loaded with {self.model.num_parameters():,} parameters\")\n",
    "        \n",
    "    def prepare_datasets(self, questions: List[str], commands: List[str], test_size: float = 0.2):\n",
    "        \"\"\"Prepare train and validation datasets\"\"\"\n",
    "        print(\"Preparing datasets...\")\n",
    "        \n",
    "        train_q, val_q, train_c, val_c = train_test_split(\n",
    "            questions, commands, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_dataset = KubernetesCommandDataset(train_q, train_c, self.tokenizer)\n",
    "        val_dataset = KubernetesCommandDataset(val_q, val_c, self.tokenizer)\n",
    "        \n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def train(self, train_dataset, val_dataset, output_dir: str = \"./k8s-command-model\"):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=1,  # Small batch size to fit in memory\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=8,  # Simulate larger batch size\n",
    "            warmup_steps=100,\n",
    "            logging_steps=10,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=50,\n",
    "            save_steps=100,\n",
    "            learning_rate=2e-5,\n",
    "            bf16=True,   # instead of fp16\n",
    "            fp16=False,  # make sure this is off\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=None,  # Disable wandb/tensorboard\n",
    "            save_total_limit=2\n",
    "        )\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"Training completed! Model saved to {output_dir}\")\n",
    "        \n",
    "    def inference(self, question: str, model_path: str = None, max_length: int = 100):\n",
    "        \"\"\"Generate kubectl command from natural language question\"\"\"\n",
    "        if model_path and not hasattr(self, 'model'):\n",
    "            self.load_trained_model(model_path)\n",
    "        \n",
    "        prompt = f\"### Instruction:\\nTranslate the following natural language request to a Kubernetes kubectl command:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                logits_processor=[SafeLogitsProcessor()],\n",
    "            )\n",
    "        \n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        command = full_response.split(\"### Response:\\n\")[-1].strip()\n",
    "        \n",
    "        return command\n",
    "    \n",
    "    def load_trained_model(self, model_path: str):\n",
    "        print(f\"Loading trained model from {model_path}...\")\n",
    "    \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "    \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "        self.model.config.use_cache = True  # inference wants cache\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    \n",
    "    def batch_inference(self, questions: List[str], model_path: str = None):\n",
    "        \"\"\"Run inference on multiple questions\"\"\"\n",
    "        if model_path:\n",
    "            self.load_trained_model(model_path)\n",
    "        \n",
    "        results = []\n",
    "        for question in questions:\n",
    "            command = self.inference(question)\n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'generated_command': command\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    trainer = KubernetesCommandTrainer()\n",
    "    \n",
    "    csv_file = \"kubernetes_commands.csv\"  # Update this path\n",
    "    questions, commands = trainer.load_data(csv_file)\n",
    "    \n",
    "    trainer.setup_model_and_tokenizer()\n",
    "    \n",
    "    train_dataset, val_dataset = trainer.prepare_datasets(questions, commands)\n",
    "    \n",
    "    trainer.train(train_dataset, val_dataset)\n",
    "    \n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "    \n",
    "    test_questions = [\n",
    "        \"View the supported API versions\",\n",
    "        \"Display information about the control plane and cluster services\",\n",
    "        \"Print the list of supported API resources\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing inference:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        command = trainer.inference(question)\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"Generated Command: {command}\")\n",
    "\n",
    "def inference_only_example():\n",
    "    \"\"\"Example of using a pre-trained model for inference only\"\"\"\n",
    "    trainer = KubernetesCommandTrainer()\n",
    "    \n",
    "    model_path = \"./k8s-command-model\"\n",
    "    trainer.load_trained_model(model_path)\n",
    "    \n",
    "    test_questions = [\n",
    "        \"How do I check the cluster information?\",\n",
    "        \"Show me the API versions\",\n",
    "        \"List all supported resources\",\n",
    "        \"Build manifests from current directory\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Inference Results:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = trainer.batch_inference(test_questions)\n",
    "    for result in results:\n",
    "        print(f\"\\nQ: {result['question']}\")\n",
    "        print(f\"A: {result['generated_command']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e64657-0e45-46fa-b991-37655186f53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.78it/s]\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "--- Generated ---\n",
      "\n",
      "[{'role': 'user', 'content': 'User question: Kubernetes command to Print the list of supported namespaced resources\\n'}, {'role': 'assistant', 'content': 'kubectl api-resources'}]\n"
     ]
    }
   ],
   "source": [
    "# !python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7bdf5c-d20a-4d19-9970-30798c84860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 24 16:37:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:53:00.0 Off |                    0 |\n",
      "|  0%   45C    P0            209W /  300W |   27707MiB /  46068MiB |     86%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e340c5-38f1-4ea4-8ffb-7ac0cacfce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'k8s lora 23-9 Claude (3).ipynb'   k8s-command-model   kubernetes_commands.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcfd92a-ec3d-4209-8b9a-40be9b4d7491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Untitled.ipynb  'k8s lora 23-9 Claude.ipynb'   kubernetes_commands.csv\n",
      " inf.py\t\t  k8s-command-model\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21db5cc4-9198-4b97-a55e-48aee0de8cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de472283bfd4b379218d88188d5aa9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Translate the following natural language request to a Kubernetes kubectl command:\n",
      "\n",
      "### Input:\n",
      "List all pods in the default namespace\n",
      "\n",
      "### Response:\n",
      "```bash\n",
      "kubectl get pods --namespace=default\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "PEFT_PATH = \"/workspace/k8s-command-model/checkpoint-150\"\n",
    "\n",
    "def load_model():\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        _ = torch.load(PEFT_PATH, map_location=\"cpu\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "def run_inference(prompt: str, max_new_tokens: int = 128):\n",
    "    tokenizer, model = load_model()\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"cuda:0\",\n",
    "    )\n",
    "\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"\"\"### Instruction:\n",
    "Translate the following natural language request to a Kubernetes kubectl command:\n",
    "\n",
    "### Input:\n",
    "List all pods in the default namespace\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    result = run_inference(prompt)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13e0e2f8-cb55-4137-b4e1-95c7c3a8d53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2694a0f257fc4c5ea4e234b1e1e03cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Translate the following natural language request to a Kubernetes kubectl command:\n",
      "\n",
      "### Input:\n",
      "Display detailed information about a specific pod\n",
      "\n",
      "### Response:\n",
      "```bash\n",
      "kubectl get pod <pod_name> -o wide\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### Instruction:\n",
    "Translate the following natural language request to a Kubernetes kubectl command:\n",
    "\n",
    "### Input:\n",
    "Display detailed information about a specific pod\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "result = run_inference(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50df3810-dda2-4f57-94d1-77cc9b15b34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd8882e2f0441ba9ec11b06edcc4150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Translate the following natural language request to a Kubernetes kubectl command:\n",
      "\n",
      "### Input:\n",
      "View logs from the first container of a job named \"batch-processing-job\"\n",
      "\n",
      "### Response:\n",
      "```\n",
      "kubectl logs job/batch-processing-job -c 0\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### Instruction:\n",
    "Translate the following natural language request to a Kubernetes kubectl command:\n",
    "\n",
    "### Input:\n",
    "View logs from the first container of a job named \"batch-processing-job\"\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "result = run_inference(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd204a60-22eb-4163-95b3-bdc83cb82e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362751c9093f4dfd8d7bd04531fac1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fb2beb1a8143ac82eabc97a4f7362e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# --- config ---\n",
    "repo_id       = \"tarun122/k8s-lora-final-2\"        \n",
    "folder_path   = \"/workspace/k8s-command-model\"  \n",
    "repo_type     = \"model\"                  \n",
    "private       = False                     \n",
    "commit_msg    = \"Upload folder\"\n",
    "path_in_repo  = \"\"                       \n",
    "\n",
    "create_repo(repo_id, repo_type=repo_type, private=private, exist_ok=True)\n",
    "\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=repo_type,\n",
    "    folder_path=folder_path,\n",
    "    path_in_repo=path_in_repo,        \n",
    "    commit_message=commit_msg,\n",
    "    # token=None                      \n",
    ")\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81777de-d1f3-435b-853e-5ac703436ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6.1G\n",
      "2.9M drwxrwxrwx 2 root root 2.9M Sep 24 16:33 checkpoint-100\n",
      " 512 -rw-rw-rw- 1 root root  184 Sep 24 16:36 generation_config.json\n",
      "1.0K -rw-rw-rw- 1 root root  867 Sep 24 16:36 config.json\n",
      "2.9M drwxrwxrwx 2 root root 2.9M Sep 24 16:36 checkpoint-150\n",
      "4.7G -rw-rw-rw- 1 root root 4.7G Sep 24 16:37 model-00001-of-00002.safetensors\n",
      "1.4G -rw-rw-rw- 1 root root 1.4G Sep 24 16:37 model-00002-of-00002.safetensors\n",
      "6.0K -rw-rw-rw- 1 root root 5.7K Sep 24 16:37 training_args.bin\n",
      " 50K -rw-rw-rw- 1 root root  50K Sep 24 16:37 tokenizer_config.json\n",
      " 17M -rw-rw-rw- 1 root root  17M Sep 24 16:37 tokenizer.json\n",
      " 512 -rw-rw-rw- 1 root root  325 Sep 24 16:37 special_tokens_map.json\n",
      " 21K -rw-rw-rw- 1 root root  21K Sep 24 16:37 model.safetensors.index.json\n",
      "4.0K -rw-rw-rw- 1 root root 3.8K Sep 24 16:37 chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "!ls -lstrh /workspace/k8s-command-model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35423de9-70ad-41cd-a0f6-837709ee0323",
   "metadata": {},
   "source": [
    "# Train 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b136305e-1203-486f-82f6-83c848017945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340935f7c87f4327a2077c88106f1520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 8,030,269,440 parameters\n",
      "Vocabulary size: 128257\n",
      "Pad token: <PAD>\n",
      "EOS token: <|eot_id|>\n",
      "Preparing datasets...\n",
      "Training samples: 399\n",
      "Validation samples: 100\n",
      "Starting memory-efficient training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "KubernetesCommandTrainer.train_memory_efficient.<locals>.CustomTrainer.training_step() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 448\u001b[39m\n\u001b[32m    444\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mgenerated_command\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# For training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     \u001b[38;5;66;03m# Uncomment below for inference only\u001b[39;00m\n\u001b[32m    451\u001b[39m     \u001b[38;5;66;03m# inference_only_example()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 402\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    399\u001b[39m train_dataset, val_dataset = trainer.prepare_datasets(questions, commands)\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_memory_efficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# Test inference\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 320\u001b[39m, in \u001b[36mKubernetesCommandTrainer.train_memory_efficient\u001b[39m\u001b[34m(self, train_dataset, val_dataset, output_dir)\u001b[39m\n\u001b[32m    311\u001b[39m trainer = CustomTrainer(\n\u001b[32m    312\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    313\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m     data_collator=data_collator,\n\u001b[32m    317\u001b[39m )\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[32m    323\u001b[39m trainer.save_model()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[31mTypeError\u001b[39m: KubernetesCommandTrainer.train_memory_efficient.<locals>.CustomTrainer.training_step() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class KubernetesCommandDataset(Dataset):\n",
    "    def __init__(self, questions: List[str], commands: List[str], tokenizer, max_length: int = 512):\n",
    "        self.questions = questions\n",
    "        self.commands = commands\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        command = self.commands[idx]\n",
    "        \n",
    "        # Format the input as instruction-following format with clear separators\n",
    "        prompt = f\"<|begin_of_text|>### Instruction:\\nTranslate the following natural language request to a Kubernetes kubectl command:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n{command}<|end_of_text|>\"\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding=False,  # Don't pad here, let the data collator handle it\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # Create labels - mask the instruction part, only train on the response\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Find the start of the response section\n",
    "        response_start_text = \"### Response:\\n\"\n",
    "        response_tokens = self.tokenizer.encode(response_start_text, add_special_tokens=False)\n",
    "        \n",
    "        # Find where the response starts in the input_ids\n",
    "        for i in range(len(input_ids) - len(response_tokens) + 1):\n",
    "            if input_ids[i:i+len(response_tokens)].tolist() == response_tokens:\n",
    "                # Mask everything before the response (set to -100)\n",
    "                labels[:i+len(response_tokens)] = -100\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "class KubernetesCommandTrainer:\n",
    "    def __init__(self, model_name: str = \"meta-llama/Llama-3.1-8B-Instruct\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def load_data(self, csv_path: str):\n",
    "        \"\"\"Load and preprocess the CSV data\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Extract questions and commands\n",
    "        questions = df['question'].tolist()\n",
    "        commands = df['command'].tolist()\n",
    "        \n",
    "        # Remove any rows with NaN values\n",
    "        valid_pairs = [(q, c) for q, c in zip(questions, commands) if pd.notna(q) and pd.notna(c)]\n",
    "        questions, commands = zip(*valid_pairs)\n",
    "        \n",
    "        print(f\"Loaded {len(questions)} training examples\")\n",
    "        return list(questions), list(commands)\n",
    "    \n",
    "    def setup_model_and_tokenizer(self):\n",
    "        \"\"\"Initialize the model and tokenizer\"\"\"\n",
    "        print(\"Loading tokenizer and model...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set proper padding token (use a different token than EOS)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            # Try to use existing special tokens first\n",
    "            if hasattr(self.tokenizer, 'unk_token') and self.tokenizer.unk_token:\n",
    "                self.tokenizer.pad_token = self.tokenizer.unk_token\n",
    "            else:\n",
    "                # Add a new padding token\n",
    "                self.tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "                \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Resize embeddings if we added new tokens\n",
    "        if self.tokenizer.pad_token == '<PAD>':\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # Enable gradient checkpointing to save memory\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.model.config.use_cache = False \n",
    "        \n",
    "        \n",
    "        print(f\"Model loaded with {self.model.num_parameters():,} parameters\")\n",
    "        print(f\"Vocabulary size: {len(self.tokenizer)}\")\n",
    "        print(f\"Pad token: {self.tokenizer.pad_token}\")\n",
    "        print(f\"EOS token: {self.tokenizer.eos_token}\")\n",
    "        \n",
    "    def prepare_datasets(self, questions: List[str], commands: List[str], test_size: float = 0.2):\n",
    "        \"\"\"Prepare train and validation datasets\"\"\"\n",
    "        print(\"Preparing datasets...\")\n",
    "        \n",
    "        # Split data\n",
    "        train_q, val_q, train_c, val_c = train_test_split(\n",
    "            questions, commands, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = KubernetesCommandDataset(train_q, train_c, self.tokenizer)\n",
    "        val_dataset = KubernetesCommandDataset(val_q, val_c, self.tokenizer)\n",
    "        \n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def train(self, train_dataset, val_dataset, output_dir: str = \"./k8s-command-model\"):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=1,  # Small batch size to fit in memory\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=8,  # Simulate larger batch size\n",
    "            warmup_steps=100,\n",
    "            logging_steps=10,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=50,\n",
    "            save_steps=100,\n",
    "            learning_rate=2e-5,\n",
    "            fp16=True,  # Use mixed precision training\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=None,  # Disable wandb/tensorboard\n",
    "            save_total_limit=2,\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"Training completed! Model saved to {output_dir}\")\n",
    "    def train_memory_efficient(self, train_dataset, val_dataset, output_dir: str = \"./k8s-command-model\"):\n",
    "        \"\"\"Memory-efficient training with proper FP16 handling\"\"\"\n",
    "        print(\"Starting memory-efficient training...\")\n",
    "        \n",
    "        # Training arguments with proper FP16 setup\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=1,  # Very small batch size\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=8,  # Simulate larger batches\n",
    "            warmup_steps=50,\n",
    "            logging_steps=5,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=25,\n",
    "            save_steps=50,\n",
    "            learning_rate=1e-5,\n",
    "            fp16=True,  # Keep FP16 for memory savings\n",
    "            fp16_full_eval=True,\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=False,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=None,\n",
    "            save_total_limit=2,\n",
    "            # Remove max_grad_norm to avoid FP16 conflicts\n",
    "            # max_grad_norm=1.0,  # Commented out\n",
    "            warmup_ratio=0.1,\n",
    "            optim=\"adamw_torch\",\n",
    "            adam_epsilon=1e-6,\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "        )\n",
    "        \n",
    "        # Custom data collator (same as before)\n",
    "        class CustomDataCollator:\n",
    "            def __init__(self, tokenizer):\n",
    "                self.tokenizer = tokenizer\n",
    "            \n",
    "            def __call__(self, features):\n",
    "                batch = {}\n",
    "                max_length = max([len(f['input_ids']) for f in features])\n",
    "                \n",
    "                batch['input_ids'] = []\n",
    "                batch['attention_mask'] = []\n",
    "                batch['labels'] = []\n",
    "                \n",
    "                for feature in features:\n",
    "                    input_ids = feature['input_ids']\n",
    "                    attention_mask = feature['attention_mask']\n",
    "                    labels = feature['labels']\n",
    "                    \n",
    "                    # Pad sequences\n",
    "                    padding_length = max_length - len(input_ids)\n",
    "                    \n",
    "                    # Pad input_ids and attention_mask\n",
    "                    padded_input_ids = torch.cat([\n",
    "                        input_ids, \n",
    "                        torch.full((padding_length,), self.tokenizer.pad_token_id)\n",
    "                    ])\n",
    "                    padded_attention_mask = torch.cat([\n",
    "                        attention_mask,\n",
    "                        torch.zeros(padding_length)\n",
    "                    ])\n",
    "                    \n",
    "                    # Pad labels (use -100 for padded positions)\n",
    "                    padded_labels = torch.cat([\n",
    "                        labels,\n",
    "                        torch.full((padding_length,), -100)\n",
    "                    ])\n",
    "                    \n",
    "                    batch['input_ids'].append(padded_input_ids)\n",
    "                    batch['attention_mask'].append(padded_attention_mask)\n",
    "                    batch['labels'].append(padded_labels)\n",
    "                \n",
    "                # Stack tensors\n",
    "                batch['input_ids'] = torch.stack(batch['input_ids'])\n",
    "                batch['attention_mask'] = torch.stack(batch['attention_mask'])\n",
    "                batch['labels'] = torch.stack(batch['labels'])\n",
    "                \n",
    "                return batch\n",
    "        \n",
    "        data_collator = CustomDataCollator(self.tokenizer)\n",
    "        \n",
    "        # Custom Trainer class to handle gradient clipping manually\n",
    "        class CustomTrainer(Trainer):\n",
    "            def training_step(self, model, inputs):\n",
    "                model.train()\n",
    "                inputs = self._prepare_inputs(inputs)\n",
    "                \n",
    "                with self.compute_loss_context_manager():\n",
    "                    loss = self.compute_loss(model, inputs)\n",
    "                \n",
    "                if self.args.n_gpu > 1:\n",
    "                    loss = loss.mean()\n",
    "                \n",
    "                if self.args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / self.args.gradient_accumulation_steps\n",
    "                \n",
    "                if self.use_apex:\n",
    "                    with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                        scaled_loss.backward()\n",
    "                else:\n",
    "                    self.accelerator.backward(loss)\n",
    "                \n",
    "                return loss.detach() / self.args.gradient_accumulation_steps\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = CustomTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the final model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"Training completed! Model saved to {output_dir}\")\n",
    "        \n",
    "    def inference(self, question: str, model_path: str = None, max_length: int = 100):\n",
    "        \"\"\"Generate kubectl command from natural language question\"\"\"\n",
    "        if model_path and not hasattr(self, 'model'):\n",
    "            self.load_trained_model(model_path)\n",
    "        \n",
    "        prompt = f\"### Instruction:\\nTranslate the following natural language request to a Kubernetes kubectl command:\\n\\n### Input:\\n{question}\\n\\n### Response:\\n\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode and extract the command\n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        command = full_response.split(\"### Response:\\n\")[-1].strip()\n",
    "        \n",
    "        return command\n",
    "    \n",
    "    def load_trained_model(self, model_path: str):\n",
    "        \"\"\"Load a previously trained model\"\"\"\n",
    "        print(f\"Loading trained model from {model_path}...\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def batch_inference(self, questions: List[str], model_path: str = None):\n",
    "        \"\"\"Run inference on multiple questions\"\"\"\n",
    "        if model_path:\n",
    "            self.load_trained_model(model_path)\n",
    "        \n",
    "        results = []\n",
    "        for question in questions:\n",
    "            command = self.inference(question)\n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'generated_command': command\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    # Initialize trainer\n",
    "    trainer = KubernetesCommandTrainer()\n",
    "    \n",
    "    # Load data (replace with your CSV file path)\n",
    "    csv_file = \"kubernetes_commands.csv\"  # Update this path\n",
    "    questions, commands = trainer.load_data(csv_file)\n",
    "    \n",
    "    # Setup model and tokenizer\n",
    "    trainer.setup_model_and_tokenizer()\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset, val_dataset = trainer.prepare_datasets(questions, commands)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train_memory_efficient(train_dataset, val_dataset)\n",
    "    \n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "    \n",
    "    # Test inference\n",
    "    test_questions = [\n",
    "        \"View the supported API versions\",\n",
    "        \"Display information about the control plane and cluster services\",\n",
    "        \"Print the list of supported API resources\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing inference:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        command = trainer.inference(question)\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"Generated Command: {command}\")\n",
    "\n",
    "def inference_only_example():\n",
    "    \"\"\"Example of using a pre-trained model for inference only\"\"\"\n",
    "    trainer = KubernetesCommandTrainer()\n",
    "    \n",
    "    # Load the trained model (update path as needed)\n",
    "    model_path = \"./k8s-command-model\"\n",
    "    trainer.load_trained_model(model_path)\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"How do I check the cluster information?\",\n",
    "        \"Show me the API versions\",\n",
    "        \"List all supported resources\",\n",
    "        \"Build manifests from current directory\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Inference Results:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = trainer.batch_inference(test_questions)\n",
    "    for result in results:\n",
    "        print(f\"\\nQ: {result['question']}\")\n",
    "        print(f\"A: {result['generated_command']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For training\n",
    "    main()\n",
    "    \n",
    "    # Uncomment below for inference only\n",
    "    # inference_only_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea5de4-2948-4872-af94-b5d60355474c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
